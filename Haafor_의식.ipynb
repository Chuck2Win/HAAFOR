{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Haafor_의식",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMERmH2QnW3EH95jqPe5Aq1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chuck2Win/HAAFOR/blob/master/Haafor_%EC%9D%98%EC%8B%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIbiYx2IWWEX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "467a1add-b84e-45a5-a82c-7f6f96d809fe"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from pandas import DataFrame as df\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import drive\n",
        "import os \n",
        "import re\n",
        "import gzip\n",
        "import csv\n",
        "drive.mount('/content/gdrive')\n",
        "new_path=('/content/gdrive/My Drive/project')\n",
        "os.chdir(new_path)\n",
        "print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UQF0m4fhVr5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "0b760dd8-6873-4270-bdaa-caa8c23db297"
      },
      "source": [
        "! pip install bpemb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bpemb\n",
            "  Downloading https://files.pythonhosted.org/packages/91/77/3f0f53856e86af32b1d3c86652815277f7b5f880002584eb30db115b6df5/bpemb-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb) (1.18.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from bpemb) (3.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bpemb) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (2.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2020.6.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (1.14.33)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (2.49.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.33 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (1.17.33)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->smart-open>=1.2.1->gensim->bpemb) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.33->boto3->smart-open>=1.2.1->gensim->bpemb) (0.15.2)\n",
            "Installing collected packages: sentencepiece, bpemb\n",
            "Successfully installed bpemb-0.3.2 sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkUL0OrtjlaW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "21abb1b7-4c55-4dea-cd04-982d377a8545"
      },
      "source": [
        "from bpemb import BPEmb \n",
        "bpemb_en=BPEmb(lang='en',dim=100, vs=5000) #100차원으로, 10000개의 단어"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mRR1ckHjUsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config(dict): \n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dimV1EgPRJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config=Config({'n_layers':2,'n_head':2,'d_model':100,'hidden_dim':4*100,'padding_idx':1,'seq_len':500,'batch_size':32,'dropout':0.3,'max_len':5000})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqq8OguFnAPe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "f96392ec-3bff-4d03-b2d4-c3af0a1fed34"
      },
      "source": [
        "# bpemb_en word2vec 저장\n",
        "bpemb_en.emb.save_word2vec_format('./pretrained/wv_5000')\n",
        "# torchtext로 구성해낸다.\n",
        "# 관건 align 작업\n",
        "vectors=torchtext.vocab.Vectors('wv_5000',cache='./pretrained') #'./wv'모델을 읽어온다.\n",
        "# 안될 때엔 변수 싹 다 지우고 다시 해라...-> drive에서"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "  0%|          | 0/5000 [00:00<?, ?it/s]Skipping token b'5000' with 1-dimensional vector [b'100']; likely a header\n",
            " 99%|█████████▉| 4942/5000 [00:00<00:00, 15584.71it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsVsIQxH55dn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8afc00d1-c66a-4ac3-a66f-e16ffb648262"
      },
      "source": [
        "# 데이터 분석하기 전에 torchtext를 활용하기\n",
        "# torchtext에 있는 사전 훈련된 임베딩 벡터 활용하기\n",
        "# torchtext iterator는 drop_last를 못한다.\n",
        "import torchtext\n",
        "TEXT=torchtext.data.Field(sequential=True,use_vocab=True,init_token='<s>',eos_token='</s>',tokenize=bpemb_en.encode,fix_length=config.seq_len,batch_first=True,lower=True,pad_token='<pad>',unk_token='<unk>') \n",
        "ISNEXT=torchtext.data.Field(sequential=False,use_vocab=False,batch_first=True,is_target=True) \n",
        "Train_data=torchtext.data.TabularDataset('./train_data_batch_swap.csv',format='csv',fields=[('A',TEXT),('B',TEXT),('NEXT',ISNEXT)],skip_header=True) # 이 때 train data는 sentence형태 이여야함.(tokenized가 되지 않은 상태)\n",
        "TEXT.build_vocab(Train_data,vectors=vectors)\n",
        "print(len(TEXT.vocab.stoi))\n",
        "config['n_token']=len(TEXT.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pt9eGIhkFHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test data\n",
        "Test_data=torchtext.data.TabularDataset('./test_data_swap.csv',format='csv',fields=[('A',TEXT),('B',TEXT),('NEXT',ISNEXT)],skip_header=True)\n",
        "# test loader\n",
        "test_loader=torchtext.data.Iterator(Test_data,batch_size=100) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0NTdEZ2r_4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# val data\n",
        "val_data=torchtext.data.TabularDataset('./val_data_swap.csv',format='csv',fields=[('A',TEXT),('B',TEXT),('NEXT',ISNEXT)],skip_header=True)\n",
        "# val loader\n",
        "val_loader=torchtext.data.Iterator(val_data,batch_size=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AelPQqp9xleg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train loader\n",
        "train_loader=torchtext.data.Iterator(Train_data,batch_size=config.batch_size) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHMc5RmglDhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model - attention model을 활용한다.\n",
        "import math\n",
        "# positional encoding\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.config=config\n",
        "        self.dropout = nn.Dropout(p=self.config.dropout)\n",
        "        self.pe = torch.zeros(config.max_len, config.d_model)\n",
        "        position = torch.arange(0, config.max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, config.d_model, 2).float() * (-math.log(10000.0) / config.d_model))\n",
        "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = self.pe.unsqueeze(0).transpose(0,1) # max len, 1, d_model\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x shape : seq len, batch size, d model\n",
        "        '''\n",
        "        self.pe=self.pe.to(x.device)\n",
        "        x = x + self.pe[:x.size(0),:,:] # 후 항 shape : seq len, 1, d model\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fda_ThXbRU_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, config,vectors):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.config=config\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
        "        self.pos_encoder = PositionalEncoding(self.config)\n",
        "        encoder_layers = TransformerEncoderLayer(self.config.d_model,self.config.n_head, self.config.hidden_dim, self.config.dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, self.config.n_layers)\n",
        "        decoder_layers = TransformerDecoderLayer(self.config.d_model,self.config.n_head, self.config.hidden_dim, self.config.dropout)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layers, self.config.n_layers)\n",
        "        self.embedding = nn.Embedding.from_pretrained(vectors,freeze=True, padding_idx=self.config.padding_idx)\n",
        "        self.fc=nn.Sequential(nn.Linear(self.config.d_model*self.config.seq_len,self.config.seq_len),nn.BatchNorm1d(self.config.seq_len),nn.ReLU(),nn.Linear(self.config.seq_len,1),nn.Sigmoid())\n",
        "        \n",
        "    def gen_padding_mask(self, input):\n",
        "        '''\n",
        "        input shape : seq len, batch size\n",
        "        embedding(input) : seq len, batch size, d_model\n",
        "        mask shape : seq len, batch size <- seq len에서 padding idx인 녀석은 1, 나머지는 0\n",
        "        근데 TransformerEncoder에 넣어주기 위해선 batch size, seq len으로 바꿔줘야 한다.\n",
        "        subsquent mask와는 다르다. -> TransformerEncoder에선 src_key_padding_mask의 input으로 들어감\n",
        "        '''\n",
        "        \n",
        "        mask=input.eq(self.config.padding_idx).T \n",
        "        return mask\n",
        "    \n",
        "    def forward(self, src,tgt):\n",
        "        device=src.device\n",
        "        src=src.T\n",
        "        tgt=tgt.T\n",
        "        src_key_padding_mask = self.gen_padding_mask(src)\n",
        "        tgt_key_padding_mask = self.gen_padding_mask(tgt)\n",
        "        self.src_key_padding_mask = src_key_padding_mask.to(device)\n",
        "        self.tgt_key_padding_mask = tgt_key_padding_mask.to(device)\n",
        "        src_out = self.embedding(src) * math.sqrt(self.config.d_model)\n",
        "        src_out = self.pos_encoder(src_out)\n",
        "        src_out = self.transformer_encoder(src_out,src_key_padding_mask=self.src_key_padding_mask)\n",
        "        tgt_out = self.embedding(tgt) * math.sqrt(self.config.d_model)\n",
        "        tgt_out = self.pos_encoder(tgt_out)\n",
        "        out1 = self.transformer_decoder(tgt_out,src_out,tgt_key_padding_mask=self.tgt_key_padding_mask,\n",
        "                                         memory_key_padding_mask=self.src_key_padding_mask)\n",
        "        out1 = out1.transpose(0,1) # batch size, seq_len, d_model\n",
        "        out2 = out1.reshape(-1,self.config.seq_len*self.config.d_model)\n",
        "        output = self.fc(out2) #  batch size, 1\n",
        "        return output.reshape(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixMtr19gqnfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('./experiment의식')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRKu8qZaqz6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    model.train() # 학습 모드를 시작합니다.\n",
        "    for _,ok in enumerate(train_loader):\n",
        "        src,tgt,is_next=ok.A,ok.B,ok.NEXT\n",
        "        src=src.to(device)\n",
        "        tgt=tgt.to(device)\n",
        "        is_next=is_next.float().to(device)\n",
        "        optimizer.zero_grad()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # clip을 해준다., 다음 번에는 안 해줘야겠다.\n",
        "        output = model(src,tgt)\n",
        "        loss = criterion(output, is_next)\n",
        "        loss.backward()\n",
        "        acc = ((output>0.5).float() == is_next).float().mean()\n",
        "        total_acc += acc.item() \n",
        "        optimizer.step()\n",
        "        n+=1\n",
        "        total_loss += loss.item()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDJ3nH_okCxq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "214b8e24-c181-401b-dda5-807c27e4548d"
      },
      "source": [
        "len(optimizer.state_dict()['param_groups']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cXiiXNJmYH4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b8ac04b-8c1d-4c10-dce9-380da8041f62"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.75]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2BiCo5LC5hE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "440ed575-76f3-4d20-c4ab-36e26560bf55"
      },
      "source": [
        "# train\n",
        "import time\n",
        "device = 'cpu'\n",
        "model = TransformerModel(config,TEXT.vocab.vectors).to(device)\n",
        "criterion = nn.BCELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "epochs = 20\n",
        "start_time = time.time()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    print(scheduler.get_last_lr()[0])\n",
        "    train()\n",
        "    scheduler.step()\n",
        "    print(optimizer.state_dict()['param_groups'][0]['lr'])                                                                                                                                                                               \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        total_loss=0.\n",
        "        total_acc=0.\n",
        "        n=0\n",
        "        for i in train_loader:\n",
        "            src,tgt,is_next=i.A,i.B,i.NEXT\n",
        "            src=src.to(device)\n",
        "            tgt=tgt.to(device)\n",
        "            is_next=is_next.float().to(device)\n",
        "            output = model(src,tgt)\n",
        "            loss = criterion(output, is_next)\n",
        "            acc = ((output>0.5).float() == is_next).float().mean()\n",
        "            total_acc += acc.item() \n",
        "            n+=1\n",
        "            total_loss += loss.item()\n",
        "        print('-' * 89)\n",
        "        print('| epoch %d | train_loss %.2f | train_acc %.2f | processed_time %.1f'%(\n",
        "                epoch, total_loss/n, total_acc/n, time.time()-epoch_start_time))      \n",
        "        print('-' * 89)\n",
        "        val_loss=0.\n",
        "        val_acc=0.\n",
        "        val_n=0\n",
        "        for i in val_loader:\n",
        "            src,tgt,is_next=i.A,i.B,i.NEXT\n",
        "            src=src.to(device)\n",
        "            tgt=tgt.to(device)\n",
        "            is_next=is_next.float().to(device)\n",
        "            output = model(src,tgt)\n",
        "            loss = criterion(output, is_next)\n",
        "            acc = ((output>0.5).float() == is_next).float().mean()\n",
        "            val_acc += acc.item() \n",
        "            val_n+=1\n",
        "            val_loss += loss.item()\n",
        "        print('-' * 89)\n",
        "            \n",
        "        print('| epoch %d | val_loss %.2f | val_acc %.2f | processed_time %.1f'%(\n",
        "                epoch, val_loss/val_n, val_acc/val_n, time.time()-epoch_start_time)) \n",
        "        print('-' * 89)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.005\n",
            "0.00475\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 1 | train_loss 28.57 | train_acc 0.51 | processed_time 111.2\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 1 | val_loss 92.99 | val_acc 0.51 | processed_time 117.3\n",
            "-----------------------------------------------------------------------------------------\n",
            "0.00475\n",
            "0.0045125\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 2 | train_loss 32.94 | train_acc 0.50 | processed_time 110.7\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 2 | val_loss 104.61 | val_acc 0.49 | processed_time 116.9\n",
            "-----------------------------------------------------------------------------------------\n",
            "0.0045125\n",
            "0.004286875\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 3 | train_loss 25.06 | train_acc 0.50 | processed_time 110.5\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 3 | val_loss 79.26 | val_acc 0.50 | processed_time 116.6\n",
            "-----------------------------------------------------------------------------------------\n",
            "0.004286875\n",
            "0.00407253125\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 4 | train_loss 22.96 | train_acc 0.50 | processed_time 110.3\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 4 | val_loss 72.09 | val_acc 0.50 | processed_time 116.5\n",
            "-----------------------------------------------------------------------------------------\n",
            "0.00407253125\n",
            "0.0038689046874999995\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 5 | train_loss 30.41 | train_acc 0.50 | processed_time 110.3\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 5 | val_loss 95.92 | val_acc 0.50 | processed_time 116.4\n",
            "-----------------------------------------------------------------------------------------\n",
            "0.0038689046874999995\n",
            "0.003675459453124999\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 6 | train_loss 23.00 | train_acc 0.54 | processed_time 110.4\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 6 | val_loss 75.13 | val_acc 0.50 | processed_time 116.5\n",
            "-----------------------------------------------------------------------------------------\n",
            "0.003675459453124999\n",
            "0.003491686480468749\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 7 | train_loss 20.58 | train_acc 0.65 | processed_time 110.4\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 7 | val_loss 71.90 | val_acc 0.52 | processed_time 116.5\n",
            "-----------------------------------------------------------------------------------------\n",
            "0.003491686480468749\n",
            "0.0033171021564453113\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 8 | train_loss 23.50 | train_acc 0.56 | processed_time 110.4\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 8 | val_loss 88.22 | val_acc 0.50 | processed_time 116.5\n",
            "-----------------------------------------------------------------------------------------\n",
            "0.0033171021564453113\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}